{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "part_04_solutions.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/iust-deep-learning/tensorflow-2-tutorial/blob/master/part_05_tf_data/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS8LoLU0OQha",
    "colab_type": "text"
   },
   "source": [
    "# Tensorflow 2.0 Tutorial: Part #5\n",
    "\n",
    "\n",
    "TensorFlow 2.0 Tutorial by IUST\n",
    "\n",
    "*   Last Update: May 2020\n",
    "*   Official Page: https://github.com/iust-deep-learning/tensorflow-2-tutorial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSj39l8gcIA8",
    "colab_type": "text"
   },
   "source": [
    "Please run the following cell before going through the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RFC1dEyMcFq2",
    "colab_type": "code",
    "cellView": "both",
    "colab": {}
   },
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Install TensorFlow\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ3pTDbtePH4",
    "colab_type": "text"
   },
   "source": [
    "### Exercise #1: Simple ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd6aCWATeWx5",
    "colab_type": "text"
   },
   "source": [
    "Convert the following dataset to the one-hot encoding format and vice-versa.\n",
    "\n",
    "a) Normal to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ra5ub8bteVKq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "NUM_CLASSES = 5\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    [0, 2, 1, 3, 3, 4, 1, 3, 3, 4])\n",
    "\n",
    "one_hot_matrix = tf.one_hot(ds, NUM_CLASSES, on_value = 1.0, off_value = 0 )\n",
    "\n",
    "a = list(ds.as_numpy_iterator())"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<TensorSliceDataset shapes: (), types: tf.int32>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/06/wq6rlwn979z9kgg2cghq7rmm0000gn/T/ipykernel_23796/1546877013.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     [0, 2, 1, 3, 3, 4, 1, 3, 3, 4])\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mone_hot_matrix\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mone_hot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNUM_CLASSES\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moff_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0msess\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 153\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001B[0m in \u001B[0;36mconvert_to_eager_tensor\u001B[0;34m(value, ctx, dtype)\u001B[0m\n\u001B[1;32m    104\u001B[0m       \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_datatype_enum\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m   \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 106\u001B[0;31m   \u001B[0;32mreturn\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEagerTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Attempt to convert a value (<TensorSliceDataset shapes: (), types: tf.int32>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>) to a Tensor."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFKWWN9Vj4lD",
    "colab_type": "text"
   },
   "source": [
    "b) one-hot to Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tfonpHDjkAn4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    [[1., 0., 0., 0., 0.],\n",
    "       [0., 0., 1., 0., 0.],\n",
    "       [0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0.],\n",
    "       [0., 0., 0., 1., 0.],\n",
    "       [0., 0., 0., 0., 1.],\n",
    "       [0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0.],\n",
    "       [0., 0., 0., 1., 0.],\n",
    "       [0., 0., 0., 0., 1.]], )\n",
    "\n",
    "####################################\n",
    "#   Put your implementation here   #\n",
    "####################################\n",
    "\n",
    "list(ds.as_numpy_iterator())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl1VeMMyoblf",
    "colab_type": "text"
   },
   "source": [
    "### Exercise #2: Seq-to-Seq dataset\n",
    "\n",
    "In this question, we want to create a `tf.data.Dataset` instance for a Sequence-to-Sequence task. The source sequence and the target sequence are both given in a separate stream of the same size. So, every data in the source stream is mapped to a sequence in the target stream at the exact position. \n",
    "\n",
    "1. Create a Dataset instance where each element is a (src_seq, tgt_seq) tuple \n",
    "2. Remove pairs that the length of the source or target sequence is zero.\n",
    "3. Batch and pad the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V3bpK-ZvsE63",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import random\n",
    "\n",
    "def get_generator(size):\n",
    "  def gen():\n",
    "    for _ in range(size):\n",
    "      yield np.random.randint(0, 100, size=[np.random.randint(0, 11)])\n",
    "  return gen\n",
    "\n",
    "src_seq_gen = get_generator(10000)\n",
    "tgt_seq_gen = get_generator(10000)\n",
    "\n",
    "src_ds = tf.data.Dataset.from_generator(\n",
    "    src_seq_gen,\n",
    "    output_types=tf.int32, output_shapes=tf.TensorShape([None]))\n",
    "\n",
    "tgt_ds = tf.data.Dataset.from_generator(\n",
    "    tgt_seq_gen,\n",
    "    output_types=tf.int32, output_shapes=tf.TensorShape([None]))\n",
    "\n",
    "ds = #...\n",
    "\n",
    "####################################\n",
    "#   Put your implementation here   #\n",
    "####################################\n",
    "\n",
    "# Hint: You may need:\n",
    "# 1. tf.shape(..), https://www.tensorflow.org/api_docs/python/tf/shape\n",
    "# 2. tf.data.Dataset.padded_batch(...),https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch\n",
    "\n",
    "for s, t in ds.take(10):\n",
    "  print(s)\n",
    "  print(t)\n",
    "  print(\"-------------\")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1DB7g6Dx_Rw",
    "colab_type": "text"
   },
   "source": [
    "### Exercise #3: Language Model Data\n",
    "\n",
    "The Language modeling task, despite being straightforward, requires special data preparation. Dataset for these tasks consists of hundreds of documents, and each document is basically a sequence of words/token. To prepare the dataset for the model, we first concatenate all these documents to form one single giant string. Then, we divide this string to equal-length smaller sequences to create elements of our dataset. Finally, the training batches are constructed by putting several elements together. For instance, suppose that a string 40 elements are created after merging all documents together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS_meTDpyFdm",
    "colab_type": "text"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://imgur.com/download/6HjGVpG\"/>\n",
    "</p>\n",
    "\n",
    "In this question, you should implement a data pipeline for a language model that follows the above strategy. That is:\n",
    "\n",
    "1. Read the dataset file. Each line is a single document.\n",
    "Tokenize each document by splitting it using a space.\n",
    "2. Append the `<eod>` token to the end of each document by which the model can learn the concept of documents.\n",
    "3. Concatenate all documents to form a single sequence of words/tokens.\n",
    "4. Divide the aforementioned sequence of tokens to a bunch of 5-element sequences.\n",
    "5. Create a batched dataset with batch_size = 4.\n",
    "\n",
    "Note that your final data pipeline should be something like this `[(batch_1_inputs, batch_1_targets), (batch_2_inputs, batch_2_targets), ..., (batch_n_inputs, batch_n_targets)]`. `targets` in the language modeling task is essentially the shifted inputs (Hint: Create another shifted dataset for targets after step 3. Then, zip the original sequence and the shifted sequence to create a dataset consists of <inputs, targets> pairs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNWhBH6s6MRx",
    "colab_type": "text"
   },
   "source": [
    "First, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpyWoZ0C6RwG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%writefile dataset.txt\n",
    "d1_w1 d1_w2 d1_w3 d1_w4 d1_w5 d1_w6 d1_w7 d1_w8 d1_w9 d1_w10\n",
    "d2_w1 d2_w2 d2_w3 d2_w4 d2_w5 d2_w6 d2_w7 d2_w8 d2_w9 d2_w10 d2_w11 d2_w12\n",
    "d3_w1 d3_w2 d3_w3 d3_w4 d3_w5 d3_w6 d3_w7 d3_w8 d3_w9 d3_w10 d3_w11 d3_w12 d3_w13 d3_w14 d3_w15\n",
    "d4_w1 d4_w2 d4_w3 d4_w4 d4_w5 d4_w6 d4_w7 d4_w8 d4_w9 d4_w10 d4_w11 d4_w12 d4_w13\n",
    "d5_w1 d5_w2 d5_w3 d5_w4 d5_w5 d5_w6 d5_w7 d5_w8 d5_w9 d5_w10 d5_w11 d5_w12 d5_w13\n",
    "d6_w1 d6_w2 d6_w3 d6_w4 d6_w5 d6_w6 d6_w7 d6_w8 d6_w9\n",
    "d7_w1 d7_w2 d7_w3 d7_w4 d7_w5 d7_w6 d7_w7 d7_w8 d7_w9 d7_w10 d7_w11 d7_w12 d7_w13\n",
    "d8_w1 d8_w2 d8_w3 d8_w4 d8_w5 d8_w6 d8_w7\n",
    "d9_w1 d9_w2 d9_w3 d9_w4 d9_w5 d9_w6 d9_w7 d9_w8 d9_w9 d9_w10 d9_w11 d9_w12\n",
    "d10_w1 d10_w2 d10_w3 d10_w4 d10_w5 d10_w6 d10_w7 d10_w8 d10_w9 d10_w10 d10_w11 d10_w12"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0AnHxuKa7CPR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "ds = tf.data.TextLineDataset('dataset.txt')\n",
    "\n",
    "# Hint: You may need: \n",
    "# 1. tf.strings.split(...) https://www.tensorflow.org/api_docs/python/tf/strings/strip\n",
    "# 2. tf.data.Dataset.flat_map(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map\n",
    "# 3. tf.data.Dataset.zip(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip\n",
    "# 4. tf.data.Dataset.skip(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip\n",
    "# 5. tf.data.Dataset.batch(...) https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "\n",
    "####################################\n",
    "#   Put your implementation here   #\n",
    "####################################\n",
    "\n",
    "ds"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXo0f6BgZhYU"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DT_2hsJ1_gl"
   },
   "source": [
    "\n"
   ]
  }
 ]
}